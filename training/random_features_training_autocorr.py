from data.dataloading import load_data, ourDataset, train_test_split
from pathlib import Path
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from models.losses.contrastive_losses import contr_loss_simple
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import time
import gc
from datetime import datetime
from metrics.evaluation import compute_eval_metrics
from models.random_features import RandomFeatures


def random_train_autocorr(num_epochs, num_patients, batch_size, hpc=False, file_format='HDF5'):
    time_run_started = datetime.now().strftime('%Y_%m_%d__%H_%M_%S')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device {device}")
    model_name = "linear"

    # Load data
    cwd = Path.cwd()  # Current working directory
    if file_format == 'zarr':
        rel_path = 'data/hcp1200.zarr.zip'
    elif file_format == 'HDF5':
        rel_path = 'data/timeseries_max_all_subjects.hdf5'
    else:
        raise NotImplementedError
    file_path = (cwd / rel_path).resolve()
    # file_path = (cwd.parent / rel_path).resolve()
    data = load_data(file_path, format=file_format, number_patients=num_patients, normalize=True, verbose=True)

    # data is a dict of numpy arrays, extract the relevant entries
    autocorr_features = data['autocorrelation_and_variation']
    same_subject = data['same_subject']
    diff_subject = data['diff_subject']
    print(f"Raw features have shape {autocorr_features.shape}")

    # Train test split with deterministic RNG
    # The seed was generated by calling
    # import secrets; secrets.randbits(128)
    data_split = train_test_split(data, perc=.75, seed=251668716030294078557169461317962359616)

    same_subject_train = data_split['train']['same_subject']
    same_subject_val = data_split['val']['same_subject']
    diff_subject_train = data_split['train']['diff_subject']
    diff_subject_val = data_split['val']['diff_subject']
    autocorr_features_train = data_split['train']['autocorrelation_and_variation']
    autocorr_features_val = data_split['val']['autocorrelation_and_variation']

    # Convert to dataset and dataloader
    dataset_train = ourDataset(autocorr_features_train, device=device)
    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)
    dataset_val = ourDataset(autocorr_features_val, device=device)
    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)

    # Free up memory
    del data

    print("here")

    # Hyperparameters
    in_dim = np.prod(autocorr_features_train.size()[-2:])  # 720 = 2 * 360
    out_dim = 1
    lr = 1e-5
    eps = 0.1

    # Instantiate model, optimiser and learning rate scheduler
    model = RandomFeatures(num_features=10)

    # Testing model output
    test_num = min(10, autocorr_features.shape[0])
    output = model(autocorr_features[:test_num].to(device))
    print(f'Tested successfully. Model output has shape {output.shape}')

    print('Evaluating model performance:')
    compute_eval_metrics(
        dataloader_train,
        same_subject_train,
        diff_subject_train,
        dataloader_val,
        same_subject_val,
        diff_subject_val,
        model_name,
        model
    )

    # Garbage collection
    del dataset_val, dataloader_val, dataset_train, dataloader_train
    gc.collect()

